{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83fcadf3",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-2/chatbot-summarization.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239436-lesson-5-chatbot-w-summarizing-messages-and-memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b651ead9-5504-45ee-938d-f91ac78dddd1",
   "metadata": {},
   "source": [
    "# Chatbot with message summarization\n",
    "\n",
    "## Review\n",
    "\n",
    "We've covered how to customize graph state schema and reducer. \n",
    " \n",
    "We've also shown a number of ways to trim or filter messages in graph state. \n",
    "\n",
    "## Goals\n",
    "\n",
    "Now, let's take it one step further! \n",
    "\n",
    "Rather than just trimming or filtering messages, we'll show how to use LLMs to produce a running summary of the conversation.\n",
    " \n",
    "This allows us to retain a compressed representation of the full conversation, rather than just removing it with trimming or filtering.\n",
    "\n",
    "We'll incorporate this summarization into a simple Chatbot.  \n",
    "\n",
    "And we'll equip that Chatbot with memory, supporting long-running conversations without incurring high token cost / latency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000a6daa-92ad-4e57-a060-d1c81176eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain_core langgraph langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09201a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import jwt\n",
    "import os,getpass\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.getenv(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "        \n",
    "def encode_jwt_token(ak, sk):\n",
    "    headers = {\n",
    "        \"alg\": \"HS256\",\n",
    "        \"typ\": \"JWT\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"iss\": ak,\n",
    "        \"exp\": int(time.time()) + 3600*24*30, # 填写您期望的有效时间\n",
    "        \"nbf\": int(time.time()) # 填写您期望的生效时间\n",
    "    }\n",
    "    token = jwt.encode(payload, sk, headers=headers)\n",
    "    return token\n",
    "\n",
    "ak = os.getenv('ak')\n",
    "sk = os.getenv('sk')\n",
    "\n",
    "model = ChatOpenAI(api_key=encode_jwt_token(ak, sk),\n",
    "    base_url=\"https://api.sensenova.cn/compatible-mode/v1/\",\n",
    "    model_name=\"SenseChat-5\",timeout=60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfddfce9-3a9b-4b35-a76d-28265515aabd",
   "metadata": {},
   "source": [
    "We'll use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "464856d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "537ade30-6a0e-4b6b-8bcd-ce90790b6392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# model = ChatOpenAI(model=\"gpt-4o\",temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3afac3-8b7a-45db-a3c1-7e4125c1bc8b",
   "metadata": {},
   "source": [
    "We'll use `MessagesState`, as before.\n",
    "\n",
    "In addition to the built-in `messages` key, we'll now include a custom key (`summary`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "948e60f0-5c76-4235-b40e-cf523205d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "class State(MessagesState):\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855ea31-5cc1-4277-a189-0b72459f67ec",
   "metadata": {},
   "source": [
    "We'll define a node to call our LLM that incorporates a summary, if it exists, into the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3f7d19b-afe0-4381-9b1a-0a832b162e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "\n",
    "# Define the logic to call the model\n",
    "def call_model(state: State):\n",
    "    \n",
    "    # Get summary if it exists\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # If there is summary, then we add it\n",
    "    if summary:\n",
    "        \n",
    "        # Add summary to system message\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "\n",
    "        # Append summary to any newer messages\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    \n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "    \n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6882042c-b42d-4d52-a6a7-6ec8efa72450",
   "metadata": {},
   "source": [
    "We'll define a node to produce a summary.\n",
    "\n",
    "Note, here we'll use `RemoveMessage` to filter our state after we've produced the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78c7aa59-3760-4e76-93f1-bc713e3ec39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_conversation(state: State):\n",
    "    \n",
    "    # First, we get any existing summary\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # Create our summarization prompt \n",
    "    if summary:\n",
    "        \n",
    "        # A summary already exists\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    # Add prompt to our history\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f982993e-f4be-4ff7-9a38-886f75398b3d",
   "metadata": {},
   "source": [
    "We'll add a conditional edge to determine whether to produce a summary based on the conversation length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b507665d-7f5d-442a-b498-218c94c5dd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "# Determine whether to end or summarize the conversation\n",
    "def should_continue(state: State):\n",
    "    \n",
    "    \"\"\"Return the next node to execute.\"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # If there are more than six messages, then we summarize the conversation\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize\"\n",
    "    \n",
    "    # Otherwise we can just end\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a838f4c-7067-4f7f-a4c4-6654e11214cd",
   "metadata": {},
   "source": [
    "## Adding memory\n",
    "\n",
    "Recall that [state is transient](https://github.com/langchain-ai/langgraph/discussions/352#discussioncomment-9291220) to a single graph execution.\n",
    "\n",
    "This limits our ability to have multi-turn conversations with interruptions. \n",
    "\n",
    "As introduced at the end of Module 1, we can use [persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/) to address this! \n",
    " \n",
    "LangGraph can use a checkpointer to automatically save the graph state after each step.\n",
    "\n",
    "This built-in persistence layer gives us memory, allowing LangGraph to pick up from the last state update. \n",
    "\n",
    "As we previously showed, one of the easiest to work with is `MemorySaver`, an in-memory key-value store for Graph state.\n",
    "\n",
    "All we need to do is compile the graph with a checkpointer, and our graph has memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d57516d-f9f1-4d3c-a84a-7277b5ce6df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"conversation\", call_model)\n",
    "workflow.add_node(\"summarize\", summarize_conversation)\n",
    "\n",
    "# Set the entrypoint as conversation\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
    "workflow.add_edge(\"summarize\", END)\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0bd5d23-ac3b-4496-a049-9a9f97d2feb9",
   "metadata": {},
   "source": [
    "## Threads\n",
    "\n",
    "The checkpointer saves the state at each step as a checkpoint.\n",
    "\n",
    "These saved checkpoints can be grouped into a `thread` of conversation.\n",
    "\n",
    "Think about Slack as an analog: different channels carry different conversations.\n",
    "\n",
    "Threads are like Slack channels, capturing grouped collections of state (e.g., conversation).\n",
    "\n",
    "Below, we use `configurable` to set a thread ID.\n",
    "\n",
    "![state.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbadf3b379c2ee621adfd1_chatbot-summarization1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2566c93b-13e6-4a53-bc0f-b00fff691d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Lance! How can I assist you today?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I am a large-scale Chinese language model based on the Transformer structure. I was developed by SenseTime and released in 2023. If you have any other questions or need more information, please feel free to ask.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That's great to hear! The San Francisco 49ers have a rich history and a passionate fan base. They've had many successful seasons and have a number of iconic players in their history. Do you have a favorite player or a memorable game that stands out to you?\n"
     ]
    }
   ],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"hi! I'm Lance\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "\n",
    "input_message = HumanMessage(content=\"what's my name?\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "\n",
    "input_message = HumanMessage(content=\"i like the 49ers!\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531e5b63-5e8b-486e-baa0-a45521e2fbc2",
   "metadata": {},
   "source": [
    "Now, we don't yet have a summary of the state because we still have < = 6 messages.\n",
    "\n",
    "This was set in `should_continue`. \n",
    "\n",
    "```\n",
    "    # If there are more than six messages, then we summarize the conversation\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "```\n",
    "\n",
    "We can pick up the conversation because we have the thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91b82aaa-17f9-49e2-9528-f4b22e23ebcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config).values.get(\"summary\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a93e9-f716-4980-8edf-94115017d865",
   "metadata": {},
   "source": [
    "The `config` with thread ID allows us to proceed from the previously logged state!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24b34f0f-62ef-4008-8e96-480cbe92ea3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Nick Bosa is indeed an outstanding player for the San Francisco 49ers, known for his exceptional skills as a defensive end. As for being the highest-paid defensive player, contracts in the NFL can change frequently, and the details often involve complex negotiations and various factors.\n",
      "\n",
      "As of my last update in 2023, Nick Bosa signed a lucrative contract extension with the 49ers, which significantly increased his earnings and solidified his status as one of the top-paid defensive players in the league. However, whether he holds the title of the highest-paid defensive player can depend on recent contracts signed by other top defensive players in the NFL.\n",
      "\n",
      "For the most current and accurate information, you might want to check the latest sports news or official NFL announcements.\n"
     ]
    }
   ],
   "source": [
    "input_message = HumanMessage(content=\"i like Nick Bosa, isn't he the highest paid defensive player?\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22f1b35f-e4bb-47f6-87b1-d84d8aed9aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Certainly! Here's a summary of our conversation:\\n\\n1. You introduced yourself as Lance.\\n2. You expressed your support for the San Francisco 49ers and mentioned that you like Nick Bosa.\\n3. We discussed Nick Bosa's status and whether he is the highest-paid defensive player in the NFL. I provided some context about his contract and noted that such rankings can change frequently based on new contracts signed by other players.\\n\\nIf you have any more questions or need further assistance, feel free to ask!\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config).values.get(\"summary\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7cc0ab-905a-4037-b7cb-69db5b89591e",
   "metadata": {},
   "source": [
    "## LangSmith\n",
    "\n",
    "Let's review the trace!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
